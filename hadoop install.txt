创建用户:
groupadd hadoop
useradd -m hadoop -g hadoop
chown -R hadoop:hadoop /home/hadoop/
echo "123456" | passwd --stdin hadoop

配置环境java hadoop:

rpm -ivh /root/jdk-8u60-linux-x64.rpm
rm -fr /usr/hadoop*
tar zxvf /root/hadoop-2.7.1.tar.gz -C /usr/
mv /usr/hadoop-2.7.1 /usr/hadoop
mkdir /usr/hadoop/tmp
chown -R hadoop:hadoop /usr/hadoop/

echo export JAVA_HOME=/usr/java/jdk1.8.0_60 >> /etc/profile
echo export JRE_HOME=/usr/java/jdk1.8.0_60/jre >> /etc/profile
echo export CLASSPATH=.:\$CLASSPATH:\$JAVA_HOME/lib:\$JRE_HOME/lib >> /etc/profile
echo export PATH=\$PATH:\$JAVA_HOME/bin:\$JRE_HOME/bin >> /etc/profile

echo export HADOOP_HOME=/usr/hadoop >> /etc/profile
echo export PATH=\$PATH:\$HADOOP_HOME/bin >> /etc/profile

source /etc/profile

配置ssh免登陆:
sed -i "s/#RSAAuthentication yes/RSAAuthentication yes/g" /etc/ssh/sshd_config 
sed -i "s/#PubkeyAuthentication yes/PubkeyAuthentication yes/g" /etc/ssh/sshd_config 
sed -i "s/#AuthorizedKeysFile/AuthorizedKeysFile/g" /etc/ssh/sshd_config
echo StrictHostKeyChecking no >> /etc/ssh/ssh_config
service sshd restart

以下操作用hadoop用户
master,slave上执行:
echo '' | ssh-keygen -t rsa -P ''
cp /home/hadoop/.ssh/id_rsa.pub /home/hadoop/.ssh/authorized_keys
将master和slave的authorized_keys合成一份

ex:
cat master的authorized_keys　>> slave的authorized_keys

chmod 700 ~/.ssh
chmod 600 ~/.ssh/authorized_keys


配置域名:
echo 192.168.0.124 master.hadoop >>/etc/hosts
echo 192.168.0.125 slave.hadoop >>/etc/hosts


配置文件修改:
vi /usr/hadoop/etc/hadoop/hadoop-env.sh 
末尾添加export JAVA_HOME=/usr/java/jdk1.8.0_60

core-site.xml
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/hadoop/tmp</value>
        <description>A base for other temporary directories.</description>
    </property>
<!-- file system properties -->
    <property>
        <name>fs.default.name</name>
        <value>hdfs://192.168.0.124:9000</value>
    </property>

	
hdfs-site.xml	
	<property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
	

mapred-site.xml	
	 <property>
        <name>mapred.job.tracker</name>
        <value>http://192.168.0.124:9001</value>
    </property>
    
启动:
hadoop namenode -format
sh start-all.sh

启动成功之后
/usr/hadoop/tmp下面有dfs文件夹

验证:
1.jps

2.hadoop dfsadmin -report

也可以通过网页查看

常用命令


hadoop dfs -mkdir /ljtest

hadoop dfs -ls /

hadoop dfs -put a.txt /ljtest

hadoop dfs -ls /ljtest

hadoop dfs -cat /ljtest/a.txt

hadoop dfs -rm /ljtest/a.txt

hadoop dfs -get /ljtest/a.txt

hadoop dfs -chmod 777 /ljtest

hadoop dfsadmin -report



